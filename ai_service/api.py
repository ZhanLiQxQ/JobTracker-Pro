# ai_service/api.py

from flask import Flask, request, jsonify
from flask_cors import CORS
import fitz  # PyMuPDF
import docx
import io
import os

# --- ä» rag_core å¯¼å…¥ç»„ä»¶ ---
from rag_core import ingest_jobs_to_vector_db, vector_store, llm
from langchain.schema import HumanMessage, SystemMessage

app = Flask(__name__)
# å…è®¸è·¨åŸŸï¼Œæ–¹ä¾¿å‰ç«¯è°ƒè¯•
CORS(app)

# ==========================================
# 1. ä¿®æ”¹å·¥å…·å‡½æ•°ï¼šä¸å†æ¥æ”¶æµï¼Œè€Œæ˜¯ç›´æ¥æ¥æ”¶ bytes æ•°æ®
# ==========================================
def extract_text(file_bytes, filename):
    """
    ä»æ–‡ä»¶å­—èŠ‚æµä¸­æå–çº¯æ–‡æœ¬
    :param file_bytes: æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹ (bytes)
    :param filename: æ–‡ä»¶å (ç”¨äºåˆ¤æ–­ç±»å‹)
    """
    text = ""
    try:
        if filename.lower().endswith('.pdf'):
            # fitz.open ç›´æ¥æ¥æ”¶ bytes
            pdf_document = fitz.open(stream=file_bytes, filetype="pdf")
            for page in pdf_document:
                text += page.get_text()
            pdf_document.close()

        elif filename.lower().endswith('.docx'):
            # python-docx éœ€è¦ä¸€ä¸ªâ€œç±»æ–‡ä»¶å¯¹è±¡â€ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦ç”¨ io.BytesIO åŒ…è£…ä¸€ä¸‹ bytes
            doc = docx.Document(io.BytesIO(file_bytes))
            for para in doc.paragraphs:
                text += para.text + '\n'
        else:
            return None
    except Exception as e:
        print(f"âŒ è§£ææ–‡ä»¶åº•å±‚æŠ¥é”™: {e}")
        # è¿™é‡Œå¯ä»¥æŠŠå…·ä½“çš„æŠ¥é”™æŠ›å‡ºå»ï¼Œæ–¹ä¾¿æ’æŸ¥
        raise e
    return text



def perform_vector_search(query_text, top_k=3):
    """
    é€šç”¨æœç´¢é€»è¾‘ï¼šåªè´Ÿè´£æŸ¥å‘é‡åº“ï¼Œä¸è´Ÿè´£ AI ç”Ÿæˆã€‚
    é€Ÿåº¦æå¿«ã€‚
    """
    if not query_text:
        return []

    print(f"ğŸ” [RAG] æ­£åœ¨æ£€ç´¢: {query_text[:50]}...", flush=True)

    # 1. å‘é‡æœç´¢
    results = vector_store.similarity_search_with_score(query_text, k=top_k)

    recommendations = []
    for doc, score in results:
        # æå–æ•°æ®
        recommendations.append({
            "job_id": doc.metadata.get('job_id'),
            "title": doc.page_content.split('\n')[0], # ç®€å•æå–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
            "description": doc.page_content,           # å®Œæ•´å†…å®¹ï¼Œå‰ç«¯ç¨åéœ€è¦ä¼ å›ç»™ AI æ¥å£
            "match_score": float(score),               # ç›¸ä¼¼åº¦åˆ†æ•°
            "url": doc.metadata.get('url'),
            "source": doc.metadata.get('source'),
            "ai_reason": None                          # å ä½ç¬¦ï¼Œç”±å‰ç«¯åç»­å¡«å……
        })

    return recommendations

# ==========================================
# 2. æ ¸å¿ƒè·¯ç”±æ¥å£
# ==========================================

# --- æ¥å£ A: æ•°æ®å…¥åº“ (çˆ¬è™«è°ƒç”¨) ---
@app.route('/rag/ingest_jobs', methods=['POST'])
def rag_ingest_jobs():
    data = request.json
    jobs = data.get('jobs') # è¿™æ˜¯ä¸€ä¸ª List

    if not jobs:
        return jsonify({"error": "No jobs provided"}), 400

    try:
        # è°ƒç”¨ rag_core é‡Œçš„å‡½æ•°å­˜å…¥ Postgres
        ingest_jobs_to_vector_db(jobs)
        return jsonify({"status": "success", "count": len(jobs)})
    except Exception as e:
        print(f"Error: {e}")
        return jsonify({"error": str(e)}), 500


# --- æ¥å£ B: æ–‡æœ¬å¿«é€Ÿæœç´¢ (å‰ç«¯ï¼šç”¨æˆ·è¾“å…¥å…³é”®è¯æ—¶è°ƒç”¨) ---
@app.route('/rag/search_only', methods=['POST'])
def search_only_endpoint():
    data = request.json
    query_text = data.get('query', '')
    top_k = data.get('k', 3)

    if not query_text:
        return jsonify({"error": "Query text is required"}), 400

    # åªæ‰§è¡Œå¿«é€Ÿæ£€ç´¢
    results = perform_vector_search(query_text, top_k)
    return jsonify({"results": results})



# ==========================================
# 2. ä¿®æ”¹è·¯ç”±æ¥å£ï¼šå…ˆè¯»å– bytesï¼Œç¡®ä¿ä¸ä¸ºç©º
# ==========================================
@app.route('/recommend_file', methods=['POST'])
def recommend_from_file():
    if 'resume_file' not in request.files:
        return jsonify({"error": "No file part"}), 400

    file = request.files['resume_file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400

    try:
        print(f"ğŸ“„ æ¥æ”¶åˆ°æ–‡ä»¶: {file.filename}")

        # --- å…³é”®ä¿®æ”¹ç‚¹ ---
        # 1. æ˜¾å¼åœ°è¯»å–æ–‡ä»¶å†…å®¹åˆ°å†…å­˜
        file_content = file.read()

        # 2. æ‰“å°æ–‡ä»¶å¤§å°ï¼Œè¿™æ˜¯æœ€é‡è¦çš„è°ƒè¯•ä¿¡æ¯ï¼
        # å¦‚æœè¿™é‡Œæ‰“å°æ˜¯ 0ï¼Œè¯´æ˜æ–‡ä»¶æ ¹æœ¬æ²¡ä¼ ä¸Šæ¥ï¼Œæˆ–è€…æµåäº†
        file_size = len(file_content)
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} bytes")

        if file_size == 0:
            return jsonify({"error": "Uploaded file is empty"}), 400

        # 3. æŠŠè¯»å¥½çš„ bytes ä¼ ç»™è§£æå‡½æ•°
        resume_text = extract_text(file_content, file.filename)

        if not resume_text:
            return jsonify({"error": "Could not extract text from file"}), 400

        print(f"âœ… æˆåŠŸæå–æ–‡æœ¬ï¼Œé•¿åº¦: {len(resume_text)}")

        # 4. å¿«é€Ÿæ£€ç´¢
        results = perform_vector_search(resume_text, top_k=5)

        return jsonify({
            "results": results,
            "extracted_text_snippet": resume_text[:200],
            "full_resume_text": resume_text
        })

    except Exception as e:
        print(f"âš ï¸ å¤„ç†è¿‡ç¨‹å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc() # æ‰“å°å®Œæ•´å †æ ˆï¼Œæ–¹ä¾¿çœ‹å“ªä¸€è¡Œé”™äº†
        return jsonify({"error": str(e)}), 500


# --- æ¥å£ D: AI è§£é‡Š (å‰ç«¯ï¼šæ‹¿åˆ°åˆ—è¡¨åï¼Œå¼‚æ­¥/æ‡’åŠ è½½è°ƒç”¨) ---
@app.route('/rag/explain_job', methods=['POST'])
def explain_job_endpoint():
    data = request.json
    # å‰ç«¯å¿…é¡»æŠŠè¿™ä¸¤æ ·ä¸œè¥¿ä¼ å›æ¥ï¼Œå› ä¸ºæœåŠ¡å™¨æ˜¯æ— çŠ¶æ€çš„
    job_desc = data.get('job_description', '')
    user_query = data.get('user_query', '') # ç”¨æˆ·çš„æœç´¢è¯ æˆ– ç®€å†å…¨æ–‡

    if not job_desc or not user_query:
        return jsonify({"error": "Missing params"}), 400

    print(f"ğŸ¤– [AI] æ­£åœ¨ç”Ÿæˆè§£é‡Š...", flush=True)

    try:
        # æ„é€  Promptï¼šé™åˆ¶å­—æ•°ï¼Œèšç„¦åŒ¹é…ç‚¹
        prompt = f"""
        ã€ç”¨æˆ·èƒŒæ™¯ã€‘
        {user_query[:600]}... (æˆªå–éƒ¨åˆ†)

        ã€ç›®æ ‡å²—ä½ã€‘
        {job_desc[:800]}... (æˆªå–éƒ¨åˆ†)

        ã€ä»»åŠ¡ã€‘
        è¯·ç”¨è‹±è¯­ï¼Œç”¨ä¸€å¥è¯ï¼ˆ50å­—ä»¥å†…ï¼‰åƒä¸“ä¸šçš„çŒå¤´é¡¾é—®ä¸€æ ·ï¼Œå‘Šè¯‰ç”¨æˆ·ä¸ºä»€ä¹ˆè¿™ä¸ªå²—ä½é€‚åˆä»–ã€‚
        è¯·ç›´æ¥è¾“å‡ºç»“è®ºï¼Œä¸è¦è¯´â€œæ ¹æ®æ‚¨çš„ç®€å†â€ä¹‹ç±»çš„åºŸè¯ã€‚
        """

        # è°ƒç”¨ LLM
        response = llm.invoke([
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªç²¾å‡†ã€å¹²ç»ƒçš„èŒä¸šé¡¾é—®ã€‚"),
            HumanMessage(content=prompt)
        ])

        return jsonify({"ai_reason": response.content})

    except Exception as e:
        print(f"âš ï¸ AI ç”Ÿæˆå¤±è´¥: {e}", flush=True)
        return jsonify({"ai_reason": "AI åˆ†ææš‚æ—¶ä¸å¯ç”¨ï¼ˆé¢åº¦ä¸è¶³æˆ–ç½‘ç»œæ³¢åŠ¨ï¼‰"})


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)